<!-- manual page source format generated by PolyglotMan v3.0.9, -->
<!-- available via anonymous ftp from ftp.cs.berkeley.edu:/ucb/people/phelps/tcltk/rman.tar.Z -->

<HTML>
<HEAD>
<TITLE>"b_eff_io"("1") manual page</TITLE>
</HEAD>
<BODY bgcolor=white>
<A HREF="#toc">Table of Contents</A><P>

<H2><A NAME="sect0" HREF="#toc0">Name</A></H2>
b_eff_io - Effective parallel MPI file I/O benchmark 
<H2><A NAME="sect1" HREF="#toc1">Description</A></H2>
The
effective I/O bandwidth benchmark (b_eff_io) covers two goals: <BR>
(1) to achieve a characteristic average number for the I/O bandwidth achievable
with parallel MPI-I/O applications <BR>
(2) to get detailed information about several access patterns and buffer
lengths. The benchmark examines "first write", "rewrite" and "read" access,
strided (individual and shared pointers) and segmented collective patterns
on one file per application and non-collective access to one file per process.
The number of parallel accessing processes is also varied and wellformed
I/O is compared with non-wellformed. On systems, meeting the rule that the
total memory can be written to disk in 10 minutes, the benchmark should
not need more than 15 minutes for a first pass of all patterns. The benchmark
is designed analogously to the effective bandwidth benchmark for message
passing (b_eff) that characterizes the message passing capabilities of
a system in a few minutes. <P>
 
<H2><A NAME="sect2" HREF="#toc2">Synopsis</A></H2>

<DL>

<DT><B>mpicc </B> </DT>
<DD><B>-o b_eff_io </B> [<B>-D WITHOUT_SHARED
</B>]  <B>b_eff_io.c -lm</B> <P>
 </DD>

<DT><B>mpirun </B> </DT>
<DD><B>-np</B><I>&nbsp;number_of_MPI_processes </I> <B>./b_eff_io </B> <B>-MB</B><I>&nbsp;number_of_megabytes_memory_per_node</I>
<B>-MT</B><I>&nbsp;number_of_megabytes_memory_of_the_total_system</I> [<B>-noshared</B>] [<B>-nounique</B>]
[<B>-rewrite</B>] [<B>-keep</B>] [<B>-N</B><I>&nbsp;<I>number_of_processes</I>[,<I>number_of_processes</I>[,...]]</I>] [<B>-T</B><I>&nbsp;scheduled_time</I>]
[<B>-p</B><I>&nbsp;path_of_fast_filesystem</I>] [<B>-i</B><I>&nbsp;info_file</I>] [<B>-e</B><I>&nbsp;number_of_errors</I>] [<B>-f</B><I>&nbsp;protocol_files'_prefix</I>]
<P>
 or  <BR>
 </DD>

<DT><B>mpiexec </B> </DT>
<DD><B>-n</B><I>&nbsp;number_of_MPI_processes </I> <B>./b_eff_io </B> <B>-MB</B><I>&nbsp;number_of_megabytes_memory_per_node</I>
<B>-MT</B><I>&nbsp;number_of_megabytes_memory_of_the_total_system</I>  [ other options see mpirun
above ]   <BR>
 <P>
 </DD>
</DL>

<H2><A NAME="sect3" HREF="#toc3">General Compile Time Options</A></H2>

<DL>

<DT><B>-D WITHOUT_SHARED</B> </DT>
<DD>to substitute the shared
file pointer by individual file pointers (implies runtime option  <B>-noshared</B>)
<P>
 </DD>
</DL>

<H2><A NAME="sect4" HREF="#toc4">Runtime Options</A></H2>

<DL>

<DT><B>-np</B><I>&nbsp;number_of_MPI_processes  </I> </DT>
<DD>(mpirun option, see man  <B>mpirun</B>
) defines the number of MPI processes started for this benchmark.  <P>
 </DD>

<DT><B>-MB</B><I>&nbsp;number_of_megabytes_memory_per_node</I>
</DT>
<DD>(mandatory) A node is defined as the unit used by or useable for one MPI
process. This value is used to compute the maximum chunk size for the patterns
1, 10, 18, 26 and 35. The maximum chunk size is defined as max( 2MB, memory
per node / 128). <P>
 </DD>

<DT><B>-MT</B><I>&nbsp;number_of_megabytes_memory_of_the_total_system</I> </DT>
<DD>(mandatory)
This value is used to compute the ratio of transferred bytes to the size
of the total memory. <P>
 </DD>

<DT><B>-noshared</B> </DT>
<DD>to substitute the shared file pointer by
individual file pointers in  pattern type 1 (implied by the compile time
option  <B>-D WITHOUT_SHARED</B>). <P>
 </DD>

<DT><B>-nounique</B> </DT>
<DD>to remove MPI_MODE_UNIQUE_OPEN on each
file opening (on some system, this option allows some MPI optimizations)
 <P>
 </DD>

<DT><B>-rewrite</B> </DT>
<DD>do rewrite between write and read for all patterns <P>
 </DD>

<DT><B>-keep</B> </DT>
<DD>to keep
<P>
all benchmarking files on close after last pattern test <P>
 </DD>

<DT><B>-N</B><I>&nbsp;<I>number_of_processes</I>[,<I>number_of_processes</I>[,...]]</I>
</DT>
<DD>defines the partition sizes used for this benchmark (default: see Default
Partition Sizes) <P>
 </DD>

<DT><B>-T</B><I>&nbsp;scheduled_time</I> </DT>
<DD>scheduled time for all partitions of
processes N (default = 1800 [seconds], see also option -N). <P>
 </DD>

<DT><B>-p</B><I>&nbsp;path_of_fast_filesystem</I>
</DT>
<DD>path of the filesystem that should be benchmarked, i.e. where this benchmarks
should write its scratch files (default is the current directory). <P>
 </DD>

<DT><B>-i</B><I>&nbsp;info_file</I>
</DT>
<DD>file containing file hints, see section Info File Format below (default
is to use no hints, i.e., to use only MPI_INFO_NULL). Using -i, the really
used hints are printed in the <I>prefix</I>.prot protocol file. The default hints
can be viewed by using -i with an empty info-file. <P>
 </DD>

<DT><B>-e</B><I>&nbsp;number_of_errors</I> </DT>
<DD>maximum
of errors printed in each pattern (default = 1). <P>
 </DD>

<DT><B>-f</B><I>&nbsp;protocol_files'_prefix</I>
</DT>
<DD>prefix of the protocol file and the summary file. The protocol and summary
will be named <I>prefix</I>.prot and <I>prefix</I>.sum (default <I>prefix</I> = b_eff_io). </DD>
</DL>

<H2><A NAME="sect5" HREF="#toc5">Remarks</A></H2>
Already
existing scratch files are automatically removed before benchmarking is
started. <P>
 If the result should be used for comparing different systems,
the benchmark is only valid if the following criterions are reached: 
<OL>
<B></B><LI>T
&gt;= 1800 sec, </LI><B></B><LI>the option  <B>-noshared </B> is NOT used, and </LI><B></B><LI>no errors are reported.
<P>
 </LI>
</OL>

<H2><A NAME="sect6" HREF="#toc6">Resources</A></H2>

<DL>

<DT>Time: </DT>
<DD>The user might expect that this benchmark would need the
scheduled time, defined with the option <B>-T</B>, or the sum of the scheduled
time values of several partitions, defined by the options <B>-T</B> and <B>-N</B>. But,
due to several reasons, this benchmark could need much more time (2x - 4x).
Reasons are: <P>
 </LI><LI>The sync operation is outside of the time-driven loop and
might consume time after the scheduled iterations. </LI><LI>The loop is finished
by an iteration that consumed much more time than the previous iterations.
</LI><LI>The pattern types 3 (segmented) and 4 (seg-coll) are not time-driven. The
estimation for adequate repeating factors is based on results with pattern
types 0-2. This estimation might be to high if the implementation of pattern
types 3 and 4 is worse than that of pattern type 0 and 2. </LI><LI>The same reason
is valid for all patterns with the access methods "rewrite" and "read".
<P>
 </DD>

<DT>Size of scratch data on disk: </DT>
<DD>On the disk defined with the option <B>-p</B>, the
size of the data written by this benchmark is about real_execution_time
*accumulated_write_bandwidth /3. <P>
 </DD>

<DT>Memory buffer space: </DT>
<DD>b_eff_io needs N
* max(4MBytes, memory_per_node/64) memory for its buffers. <P>
 </DD>
</DL>

<H2><A NAME="sect7" HREF="#toc7">Postprocessing</A></H2>
<B>beffio_eps</B>
generates diagrams of the summary protocol file given. <P>
 Synopsis: beffio_eps
[ <I>protocol_file_prefix</I> ] (default=b_eff_io) <P>
 Output: 
<DL>

<DT>black/white diagrams,
e.g., for publication: </DT>
<DD><I><A HREF="prefix_.np">prefix</I>_(np)</A>
_(write, rewrt, read, type0_sca, type1_sha,
type2_sep, type3_seg and type4_sgc)_mono.eps, <I>prefix</I>_(write, rewrt, read)_mono.eps
<P>
 </DD>

<DT>if dvips is available, a summary sheet of these diagrams: </DT>
<DD><I><A HREF="prefix_.np">prefix</I>_(np)</A>
_on1page.ps
<P>
 </DD>

<DT>colored diagrams with thick lines, e.g., for slides: </DT>
<DD><I><A HREF="prefix_.np">prefix</I>_(np)</A>
_(write,
rewrt, read, type0_sca, type1_sha, type2_sep, type3_seg and type4_sgc)_color.(eps
and png), <I>prefix</I>_(write, rewrt, read)_color.(eps and png) </DD>
</DL>

<H2><A NAME="sect8" HREF="#toc8">Examples</A></H2>
<BR>
<PRE> CRAY T3E:  
   Prerequisites: using moduls mpt
   Compilation on T3E :
     cc -o b_eff_io -D WITHOUT_SHARED b_eff_io.c
     cc -o b_eff_io -D WITHOUT_SHARED b_eff_io.c \
        ../ufs_t3e/ad_ufs_open.o ../ufs_t3e/ad_ufs_read.o \
        ../ufs_t3e/ad_ufs_write.o
     cc -o b_eff_io -D WITHOUT_SHARED b_eff_io.c \
        ../ufs_t3e/ad_ufs_*.o
   Execution: export MPI_BUFFER_MAX=4099
   T3E-900 with 128 MB/processor and 512 PEs:
     mpirun -np 64 ./b_eff_io -MB 128 -MT 65536 \
            -p $SCRDIR -f b_eff_io_T3E900_064PE
   T3E-1200 with 512 MB/processor and 512:
     mpirun -np 64 ./b_eff_io -MB 512 -MT 262144 \
            -p $SCRDIR -f b_eff_io_T3E1200_064PE
 
 SX-4:
   Prerequisites: -
   Compilation on SX-4/32 with 256 MB/processor:
     mpicc -o b_eff_io b_eff_io.c -lm
   Execution:
     mpirun -np 8 ./b_eff_io -MB 256 -MT 8192 \
            -p $SCRDIR -f b_eff_io_SX4_08PE
 
 Postprocessing (on local workstation):
     b_eff_io_eps 64 b_eff_io_T3E900_064PE
     b_eff_io_eps 64 b_eff_io_T3E1200_064PE
     b_eff_io_eps  8 b_eff_io_SX4_08PE
 
 Outputfiles:
     b_eff_io_T3E900_064PE.sum       human readable summary
                          .prot      full benchmark protocol
                          _*_mono.eps   diagrams black/white
                          _*_color.eps  colored, for slides
     Same for b_eff_io_T3E1200_064PE and b_eff_io_SX4_08PE.
</PRE><P>
 
<H2><A NAME="sect9" HREF="#toc9">See Also</A></H2>
<P>
<B><A HREF="mpi.1">mpi</B>(1)</A>
, <B><A HREF="mpirun.1">mpirun</B>(1)</A>
,<B></B> <B><A HREF="mpiexec.1">mpiexec</B>(1)</A>
,<B></B> <B><A HREF="mpicc.1">mpicc</B>(1)</A>
,<B></B> <B>www.hlrs.de/mpi/b_eff_io/</B>
<B>www.hlrs.de/mpi/b_eff/</B> <B>www.hlrs.de/mpi/mpi_t3e.html#StripedIO</B> <P>

<HR><P>
<A NAME="toc"><B>Table of Contents</B></A><P>
<UL>
<LI><A NAME="toc0" HREF="#sect0">Name</A></LI>
<LI><A NAME="toc1" HREF="#sect1">Description</A></LI>
<LI><A NAME="toc2" HREF="#sect2">Synopsis</A></LI>
<LI><A NAME="toc3" HREF="#sect3">General Compile Time Options</A></LI>
<LI><A NAME="toc4" HREF="#sect4">Runtime Options</A></LI>
<LI><A NAME="toc5" HREF="#sect5">Remarks</A></LI>
<LI><A NAME="toc6" HREF="#sect6">Resources</A></LI>
<LI><A NAME="toc7" HREF="#sect7">Postprocessing</A></LI>
<LI><A NAME="toc8" HREF="#sect8">Examples</A></LI>
<LI><A NAME="toc9" HREF="#sect9">See Also</A></LI>
</UL>
</BODY></HTML>
